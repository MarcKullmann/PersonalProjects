---
title: "Data Wrangling, randomForest, GradientBoosting"
author: "Marc Kullmann"
date: "`r Sys.Date()`"
output:
  html_document:
fontsize: 11pt
always_allow_html: yes
urlcolor: blue
---
\tableofcontents
\newpage 

```{r setup, include=F}
knitr::opts_chunk$set(cache = TRUE)
library(scales)
library(broom)
library(randomForest)
library(lubridate)
library(readr)
library(reshape2)
library(tidyverse)
library(geosphere)
library(leaflet)
library(missForest)
library(doParallel)
library(caret)
library(mice)
library(VIM)
library(micemd)
library(parallel)
library(tseries)
```


# Introduction
This markdown is a small attempt to get used to the evironment of R and the machine learning algorithm of random forest, hereby I used the dataset from kaggle (https://www.kaggle.com/ruiqurm/lianjia). As one can find, several enthusiasts have conducted comprehensive analysis on this dataset, however, I have worked mostly on my own.

# Data Overview and Cleaning
## Import data, declare tibble, overview
First of all, we have to import the data file. For convenience, I choose to declare the CSV file as a tibble dataframe. 
```{r Read the data, warning=FALSE}
Df <- read.csv("new.csv", 
               sep=',', fileEncoding="latin1")
data <- as_tibble(Df)
```
Get a quick overview of the underlaying data set. 
```{R Overview, warning=FALSE}
str(data)
head(data)
```
 
## Missing Data
```{r Missing data, warning=FALSE}
#Tidy Data: Find all the missing Data in variables
missing <- tibble(na = sapply(data, function(x) any(is.na(x) | is.infinite(x))),
                  sum_na = sapply(data, function(x) sum(is.na(x))),
                  name = colnames(data)) %>% 
  filter(na == TRUE)
missing
```
As we can see 6 variables contain missing data, with the according amount. Important to notice is that,  almost 50 % of the variable of DOM are missing, hence we cannot just drop the missing observations from this particular variable, but from the others. Further, we are looking into DOM, to determine how to replace the big amount of NA's.
```{r NA-treatment, warning=FALSE}
# # Tidy Data: Drop all the missing Data except DOM
# data <- data %>% drop_na(-DOM)
# 
# plot(density(data$DOM, na.rm = T))
```

## Data Cleaning
As we know, the variable DOM has lots of missing values, hence we follow the siggestion of Mr. Bouchet and replace the missing values with the median. Furthermore, we extract the floor number of the wrongly imported floor variable.
```{r Mutation of data, warning=FALSE}
# Replace NaN of DOM with median and change certian variables as numeric.
data %>% select(c(tradeTime, totalPrice, price, square)) %>% summary(.)

data <- mutate(data,
               url = as.character(url),
               id = as.character(id),
               followers = as.numeric(followers), 
               price = as.numeric(price))

# change rooms
data %>% select(c(floor, livingRoom, drawingRoom, kitchen, bathRoom)) %>% summary(.)

data <- mutate(data,
               floor = as.numeric((str_extract(floor, "[0-9]+"))),
               livingRoom = as.numeric((str_extract(livingRoom, "[0-9]+"))), 
               drawingRoom = as.numeric((str_extract(drawingRoom, "[0-9]+"))),
               kitchen = as.numeric((str_extract(kitchen, "[0-9]+"))),
               bathRoom = as.numeric((str_extract(bathRoom, "[0-9]+"))))
```

## Categorical Variable Adjustment
Now we classify each categorical variable into the according group (levels/factors) 
```{r Level adjustments and mutation, warning=FALSE}
data %>% select(c(buildingType, constructionTime, renovationCondition, buildingStructure, elevator, fiveYearsProperty, ladderRatio)) %>% summary(.)

# Generate Grouping-Functions:

# Buildingtype names
makeBuildingType <- function(x){
  if(!is.na(x)){
    if(x==1){
      return('Tower')
    }
    else if (x==2){
      return('Bungalow')
    }
    else if (x==3){
      return('Mix_plate_tower')
    }
    else if (x==4){
      return('plate')
    }
    else return('wrong_coded')
  }
  else{return('missing')}
}

# Renovationcondition Names
makeRenovationCondition <- function(x){
  if(x==1){
    return('Other')
  }
  else if (x==2){
    return('Rough')
  }
  else if (x==3){
    return('Simplicity')
  }
  else if (x==4){
    return('Hardcover')
  }
  else{return('missing')}
}

# Buldingstructure Names  
makeBuildingStructure <- function(x){
  if(x==1){
    return('Unknown')
  }
  else if (x==2){
    return('Mix')
  }
  else if (x==3){
    return('Brick_Wood')
  }
  else if (x==4){
    return('Brick_Concrete')
  }
  else if (x==5){
    return('Steel')
  }
  else if (x==6){
    return('Steel_Concrete')
  }
  else{return('missing')}
}

# make District names
makeDistrict <- function(x){
  if(!is.na(x)){
    if(x==1){
      return('Dong Cheng')
    }
    else if (x==2){
      return('Chong Wen & Xuan Wu')
    }
    else if (x==3){
      return('Feng Tai')
    }
    else if (x==4){
      return('Da Xing')
    }
    else if (x==5){
      return('Fang Shan')
    }
    else if (x==6){
      return('Chang Ping')
    }
    else if (x==7){
      return('Chao Yang')
    }
    else if (x==8){
      return('Hai Dian')
    }
    else if (x==9){
      return('Shi Jing Shan')
    }
    else if (x==10){
      return('Xi Cheng')
    }
    else if (x==11){
      return('Tong Zhou')
    }
    else if (x==12){
      return('Men Tou Gou')
    }
    else if (x==13){
      return('Shun Yi')
    }
    else return('wrong_coded')
  }
  else{return('missing')}
}

# Mutate rest of the Variables into categorical variables:
data <- mutate(data, 
               buildingType = sapply(buildingType, makeBuildingType),
               renovationCondition = as.factor(sapply(renovationCondition,
                                                      makeRenovationCondition)),
               buildingStructure = sapply(buildingStructure, 
                                          makeBuildingStructure),
               subway = ifelse(subway == 1, 'has_subway', 'no_subway'),
               fiveYearsProperty = ifelse(fiveYearsProperty == 1, 
                                          'owner_less_5y', 'owner_more_5y'),
               elevator = ifelse(elevator == 1, 'has_elevator' , 'no_elevator'),
               district = sapply(district, makeDistrict))

# change building related attributes
data <- mutate(data,
               buildingType = as.factor(buildingType),
               buildingStructure = as.factor(buildingStructure),
               elevator = as.factor(elevator),
               fiveYearsProperty = as.factor(fiveYearsProperty),
               ladderRatio = as.numeric(ladderRatio),
               renovationCondition = as.factor(renovationCondition),
               subway = as.factor(subway),
               district = as.factor(district))

missing2 <- tibble(na = sapply(data, function(x) any(is.na(x) | is.infinite(x))),
                   sum_na = sapply(data, function(x) sum(is.na(x))),
                   name = colnames(data)) %>% 
  filter(na == TRUE)
missing2
```
As we can see some missing data appeared as we set our categorical variables and separated some of the variables.

To make use of the time stamp, we generate the floor dates of the year, month and day. Additionaly calculate the weekday on which the most "transactions" happen (in our case taken offline from the webpage). 

To incorporate the geo information of each object. As we have the longitude and latitude coordinates, we can calculate the distance for each object from the city center of Beijing. The coordinates of the city center are followed by the webpage wikipedia:

39.9042ยบ N, 116.4074ยบ E

```{r Time and Distance}
## Adjusting the time variables
# Declare tradeTime as Date, 
# extract floor dates: year, month, day 
# Adjust constructionTime and generate buldingAge
data <- mutate(data, 
               tradeTime = as_datetime(tradeTime),
               tradeYear = floor_date(tradeTime, unit = "year"),
               tradeMonth = floor_date(tradeTime, unit = "month"),
               tradeDay = floor_date(tradeTime, unit = "day"),
               tradeDays = as.numeric(format(tradeTime, format="%d")),
               constructionTime = as.numeric((str_extract(constructionTime, "[0-9]+"))))

# Distance calculation via harversine

bj_lat <- 39.9042
bj_log <- 116.4074

data <- data %>% 
  mutate(distance = distHaversine(cbind(Lng, Lat), cbind(bj_log, bj_lat), r=6378137))

missing3 <- tibble(na = sapply(data, function(x) any(is.na(x) | is.infinite(x))),
                   sum_na = sapply(data, function(x) sum(is.na(x))),
                   name = colnames(data)) %>% 
  filter(na == TRUE)
missing3
```
### Missing variables treatment
As we have seen in 'missing3' there are several variables with missing values, lets see whether there is a pattern behind the missing values
```{r}
# na_pattern <- md.pattern(data)
aggr(data, col = c('blue', 'yellow'),
     numbers = T, sortVars = T,
     labels = names(data), cex.axis = 0.5)
```
Given the plot of the aggregated data, there is hardly any pattern of missing data. 

Because of the vast options of treatments, I will first compare mean and median imputation for DOM and constructionTime. Other variable seem relatively insigificant, as their numbers are in the permille level.
```{r mean and median imputation}
data2<- mutate(data,
               DOM_mean = ifelse(is.na(DOM), mean(DOM,na.rm=T), DOM),
               DOM_median = ifelse(is.na(DOM), median(DOM,na.rm=T), DOM),
               constructionTime_mean = ifelse(is.na(constructionTime), mean(constructionTime,na.rm=T), constructionTime),
               constructionTime_median = ifelse(is.na(constructionTime), median(constructionTime,na.rm=T), constructionTime))

ggplot(data = data2) +
  geom_density(aes(x = DOM, color = "DOM")) +
  geom_density(aes(x = DOM_mean, color = "DOM_mean")) +
  geom_density(aes(x = DOM_median, color = "DOM_median"))
ggplot(data = data2) +
  geom_density(aes(x = constructionTime, color = "constructionTime")) +
  geom_density(aes(x = constructionTime_mean, color = "constructionTime_mean")) +
  geom_density(aes(x = constructionTime_median, color = "constructionTime_median"))
```
Given the DOM plot, we can see a lot changes from the imputation of mean or median. Within the constructionTime we can see minor changes, which do not create any significant bias. Hence the situation with DOM, we will implement an imputation via 'cart' of the MICE package.

```{r mice imputation}
# remove irrelevant data and prepare for imputation
data_for_imput <- data %>% 
  select(-c(url, Cid, price, followers, ladderRatio, distance, drawingRoom, kitchen, 
            tradeTime, tradeYear, tradeMonth, tradeDay, tradeDays)) %>%
  as.data.frame()

# initiate imputation
start_mice <- Sys.time()
imputed <- mice.par(data_for_imput, m = 1, maxit = 5, method = 'cart', seed = 123)
time_mice <- Sys.time() - start_mice
time_mice

# extract
imputed_data <- complete(imputed, 1)

# combine
extracted <- select(data, 
                c(url, id, Cid, price, followers, ladderRatio, distance, drawingRoom, 
                  kitchen, tradeTime, tradeYear, tradeMonth, tradeDay, tradeDays))
data_analysis <- left_join(imputed_data, extracted, by = "id") 

ggplot(data = data2) +
  geom_density(aes(x = DOM, color = "DOM")) +
  geom_density(aes(x = DOM_mean, color = "DOM_mean")) +
  geom_density(aes(x = DOM_median, color = "DOM_median")) +
  geom_density(aes(x = data_analysis$DOM, color = "DOM_imputed"))
ggplot(data = data2) +
  geom_density(aes(x = constructionTime, color = "constructionTime")) +
  geom_density(aes(x = constructionTime_mean, 
                   color = "constructionTime_mean")) +
  geom_density(aes(x = constructionTime_median, 
                   color = "constructionTime_median")) +
  geom_density(aes(x = data_analysis$constructionTime, 
                   color = "constructionTime_imputed"))

tibble(na = sapply(data_analysis, function(x) any(is.na(x) | is.infinite(x))),
                   sum_na = sapply(data_analysis, function(x) sum(is.na(x))),
                   name = colnames(data_analysis)) %>% 
  filter(na == TRUE)
```
As shown by the last two plots, the imputed values look much smoother, compared to the mean/median solution. And all missing values have been replaced.

# Exploratory Data Analysis
## Summary
Summarize all key variables
```{r summary of key variables, warning=FALSE}
# Summary of key variables, excluding dates and factors.
df_sum <- data_analysis %>%
  select(-c(url, id, Lng, Lat, Cid, tradeTime, buildingType, constructionTime, 
            renovationCondition, buildingStructure, elevator, fiveYearsProperty, 
            subway, district)) %>%
        summarise_each(funs(min = min, 
                      q25 = quantile(., 0.25), 
                      median = median, 
                      q75 = quantile(., 0.75), 
                      max = max,
                      mean = mean, 
                      sd = sd))

df_sum <- df_sum %>% gather(stat, val) %>%
                  separate(stat, into = c("var", "stat"), sep = "_") %>%
                  spread(stat, val) %>%
  rename(name = var)

df_sum
```
## Frequency Plots
```{r time frequency monthly}
## Monthly

# check how many transactions per month
cat("Complete Set:")
table(data_analysis$tradeYear)

data_analysis <- data_analysis %>%  
  filter(tradeYear > "2011-01-01" & tradeYear < "2018-01-01") %>%
  filter(tradeMonth > "2011-01-01" & tradeMonth < "2018-01-01") 
cat("\nTruncated Set:")
table(data_analysis$tradeYear)

ggplot(data = data_analysis) +
  geom_freqpoly(aes(x = as_date(tradeMonth), color = "Total"), bins = 84) +
  geom_freqpoly(aes(x = as_date(tradeMonth), color = district), bins = 84) +
  scale_x_date(date_breaks = "6 month", date_labels = "%Y - %m") +
  #coord_flip() +
  theme(axis.text.x = element_text(angle = -45, 
                                   vjust = 1, 
                                   hjust = 0),
        legend.title = element_blank(),
        legend.position = "bottom") +
  labs(title = "Number of Transactions per Month",
       y = "Number of Transactions",
       x = "Month")
```

```{r time frequency yearly}
## Yearly 

ggplot(data = data_analysis) +
  geom_freqpoly(aes(x = as_date(tradeYear), color = "Total"), bins = 7) +
  geom_freqpoly(aes(x = as_date(tradeYear), color = district), bins = 7) +
  scale_x_date(date_breaks = "year", date_labels = "%Y") +
  #coord_flip() +
  theme(axis.text.x = element_text(angle = -45, 
                                   vjust = 1, 
                                   hjust = 0),
        legend.title = element_blank(),
        legend.position = "bottom") +
  labs(title = "Number of Transactions per Year",
       y = "Number of Transactions",
       x = "Quarters")
```

## Plot average daily Price
Now we graph the average daily price. To plot our data more intuitively we omit trades before 2009, as there are only few observations. continuing with saving the plot in the Data priceplot.
```{r Avg. Daily Price Plot, warning=FALSE}
# Calculate average daily price
avg_price <- data %>% group_by(tradeTime) %>% 
                    summarize(mean_price = mean(totalPrice, na.rm = TRUE)) %>%
                    mutate(year = format(tradeTime, format="%Y"))

# Plot average daily price from 2010 on
filter(avg_price, year > 2009) %>% 
  ggplot(avg_price, mapping = aes(x = tradeTime, y = mean_price)) +
  geom_point(aes(colour=mean_price), alpha=.25) +
  labs(
    title='Daily Average Total Price of Traded Homes, from 2010 to 2018',
    x = "Date", 
    y = "Average total Price per Day, in ten thousand RMB", 
    colour = "Mean Price") +
  scale_colour_gradient(low = "black", high = "blue1") +
  scale_radius(range=c(1,10))
  ggsave("DailyAvg.Price.pdf")
```
Interesting in this plot is, that we cann see that the average daily price rise over the course of approximately eight years. Although, this is only a rough estimation, as we don't know where and which objects were sold.

```{r  Locations Map, warning=FALSE}
data_analysis_sample <- sample_n(data_analysis, 10000, replace = FALSE)

pal <- colorFactor(palette = topo.colors(nlevels(data$district)), domain = data$district[1:13])
leaflet(data_analysis_sample) %>% 
  #addTiles() %>%
  addProviderTiles(providers$CartoDB.Positron) %>%
  setView(lng=116.4074, lat=39.9042, zoom = 8) %>%
  addCircleMarkers(
    ~Lng, ~Lat,
    radius = 2,
    color = ~pal(district),
    stroke = T, 
    fillOpacity = 0.7,
    popup= ~district
    # clusterOptions = markerClusterOptions()
  ) %>%
  addLegend("bottomright", pal = pal, values = ~district,
            title = "District by Color",
            opacity = 1
  )
```
```{r totalPrice Map, warning=FALSE}
pal2 <- colorNumeric(c("blue", "red"), 
                     domain = data_analysis_sample$totalPrice)
leaflet(data_analysis_sample) %>%   
  addProviderTiles(providers$CartoDB.Positron) %>%
  setView(lng=116.4074, lat=39.9042, zoom = 8) %>%
  addCircleMarkers(
    ~Lng, ~Lat,
    radius = 2,
    color = ~pal2(totalPrice),
    #stroke = T, 
    #fillOpacity = 0.7,
    popup= ~totalPrice
    # clusterOptions = markerClusterOptions()
  ) %>%
  addLegend("bottomright", pal = pal2, values = ~totalPrice,
            title = "totalPrice by Color",
            opacity = 1
  )
```


## Price Evolution for each District by Year
```{r Monthly Average Price per District, warning=FALSE}
# Monthly Average Price per District
data_analysis %>% filter(tradeYear > 2010) %>%
  group_by(month=floor_date(tradeTime, "month"), district) %>%
  summarize(summary_variable=mean(totalPrice)) %>%
  ggplot(aes(month, summary_variable, color = district)) +
  geom_line() +
  facet_wrap( ~ district, ncol = 2) +
  labs(title = "Monthly Average Price per District",
       subtitle = "Data plotted by Month",
       y = "Average Total Price",
       x = "Year") +
  theme(legend.position = "none")
# ggsave("map_avgpriceperdistrict.png")
```

```{r Total Price Change per District, warning=FALSE}
# Total Price Change per District (absolute)
data_analysis %>% filter(tradeYear > 2010) %>%
  group_by(tradeYear, district) %>%
  ggplot() +
  geom_boxplot(aes(x = reorder(district, -totalPrice), 
             y = totalPrice, 
             color = district)) +
  scale_y_log10() +
  theme(axis.text.x = element_text(angle = -45, 
                                   vjust = 1, 
                                   hjust = 0),
        legend.position = "none",
        axis.title.x = element_blank()) +
#  coord_flip() +
  labs(title = "Total Price per District, in log10()",
       y = "Total Price")
# ggsave("map_priceperdistrict.png")
```

## Regressions
The nature of this data set is obviously a time series, although I need to say, I have not implmented my time series analysis yet. Furthermore, this analysis is more orientated towards the randomForest part.

### TTest
```{r}
ttest <- data_analysis %>% 
  nest(-district) %>% 
  mutate(fit = map(data, ~t.test(.$totalPrice)),
         p   = map_dbl(fit, "p.value"),
         results = map(fit, glance)) %>% 
  unnest(results)
ttest
```


### Linear Regression
```{r Linear Regression, warning=FALSE}
lin_reg <- data_analysis %>%
  nest(-district) %>%
  mutate(fit = map(data, ~ lm(totalPrice ~ 
                                DOM +  livingRoom +  drawingRoom +  kitchen + 
                                bathRoom + floor +  buildingType +
                                buildingStructure + elevator +  fiveYearsProperty +
                                subway +  factor(tradeYear) + distance + followers, 
                              data = .)),
         glance = map(fit, glance),
         augment = map(fit, augment),
         tidy = map(fit, tidy))

lin_reg %>% unnest(glance) %>%
ggplot(data = ) +
  geom_bar(aes(x = factor(district), y = r.squared), stat = "identity") + 
  labs(x = "District", y = expression(R^{2})) +
  theme(axis.text.x = element_text(angle = -45, vjust = 1, hjust = 0))
  # ggsave("rsqrtperdistrict.pdf" )
```
As we can see FengTai has the highest R-squared value, lets see how the variables interact with the totalPrice:
```{r FengTai Results}
# Feng Tai
lin_reg_fit <- lin_reg$fit
summary(lin_reg_fit[[10]])
```

Now we try the nested randomForest algorithm

### RandomForest Regression
```{r Random Forest, warning=FALSE}
# Convenience function to get importance information from a randomForest fit
# into a dataframe
imp_df <- function(rf_fit) {
  imp <- importance(rf_fit)
  vars <- rownames(imp)
  imp %>% 
    tibble::as_tibble() %>% 
    mutate(var = vars)
}

# Take only 75000 observations as my computing power is limited
data_analysis_sample <- sample_n(data_analysis, size = 75000)

set.seed(123)
start_rF <- Sys.time()
rF <- data_analysis_sample %>% 
  # Selecting data to work with
  na.omit() %>%
  select(totalPrice, district,
           DOM, livingRoom, drawingRoom, kitchen, bathRoom,
           floor, buildingType, buildingStructure,
           elevator, fiveYearsProperty, subway, tradeYear,
           distance, followers) %>%
  # Nesting data and fitting model
  nest(-district) %>% 
  mutate(fit = map(data, ~ randomForest(totalPrice ~ ., data = .,
                                        importance = TRUE,
                                        ntree = 100)),
         importance = map(fit, imp_df)) %>% 
  # Unnesting and plotting
  unnest(importance)
time_rF <- Sys.time() - start_rF
time_rF


# Plot each feature and its importance separated with each district to 
# understand how each district is different
rFPlot <- ggplot(rF, aes(x = `%IncMSE`, y = var, color = `%IncMSE`)) +
  geom_segment(aes(xend = min(`%IncMSE`), yend = var), alpha = .2) +
  geom_point(size = 3) +
  facet_grid(. ~ district) +
  guides(color = "none") +
  theme_bw() +
  labs(y = "Variable",
       x = "Importance")
rFPlot
```
### Gradient Boosting Time Series Analysis
```{r}
# Generate a daily time series
data_ts_analysis <- data_analysis %>% 
  group_by(tradeDay) %>%
  summarise(totalPrice_sum = sum(totalPrice))

# splitting into train and test
data_ts_analysis_index <- createDataPartition(data_ts_analysis$totalPrice_sum, p = .8, list = F)

ts_train <- data_ts_analysis[data_ts_analysis_index,]
ts_test <- data_ts_analysis[-data_ts_analysis_index,]

# train control for time slices
myTimeControl <- trainControl(
  method = "timeslice",
  initialWindow = 50,
  horizon = 1,
  fixedWindow = TRUE
)

# train grid for gbm
grid_gbm <- expand.grid(
  n.trees = c(100, 250, 500),
  shrinkage = c(0.001, 0.01),
  interaction.depth = c(1, 16, 20),
  n.minobsinnode = c(1, 2, 4)
)

#find best tune
cl <- makePSOCKcluster(10)
registerDoParallel(cl)
start_gbm <- Sys.time()
gbmts_train <- train(
  totalPrice_sum ~ tradeDay,
  data = ts_train,
  method = "gbm",
  distribution = "gaussian",
  trControl = myTimeControl,
  verbose = FALSE,
  tuneGrid = grid_gbm,
  preProc = c("center", "scale"))
time_gbm <- Sys.time() - start_gbm
stopCluster(cl)

time_gbm
gbmts_train$bestTune

# run best tune
cl <- makePSOCKcluster(10)
registerDoParallel(cl)
gbmts_train2 <- train(
  totalPrice_sum ~ tradeDay,
  data = ts_train,
  method = "gbm",
  distribution = "gaussian",
  trControl = myTimeControl,
  verbose = FALSE,
  tuneGrid = gbmts_train$bestTune,
  preProc = c("center", "scale"))
stopCluster(cl)

# predict values
gbmts_predict <- predict(
  gbmts_train2,
  newdata = ts_test
)

# plot values
ggplot(data = ts_test, aes(x = tradeDay, y = totalPrice_sum)) +
  geom_line(aes(color = "actual")) +
  geom_line(aes(y = gbmts_predict, x = tradeDay, 
                color = "predict")) +
  labs(title = "Gradient Boosting Test Run of the Daily TimeSeries",
       subtitle = "Aggregated price of each trading day",
       y = "Sum of totalPrice",
       x = "Time")
```


# References
*  [Housing price in Beijing](https://www.kaggle.com/ruiqurm/lianjia)
*  [Forecasting Beijing's housing prices](https://www.kaggle.com/jonathanbouchet/forecasting-beijing-s-housing)
